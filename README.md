# NumNet
ğŸ§  Model: Basic feedforward neural network implemented from scratch for MNIST digit classification.
ğŸ“Š Input layer: 784 neurons (28x28 pixel images flattened)
ğŸ”® Hidden layer: Single layer with customizable number of neurons (e.g., 128 or 256)
ğŸ”¢ Output layer: 10 neurons (one for each digit 0-9)
ğŸ”— Activation functions: ReLU for hidden layer, softmax for output layer
ğŸ§® Implementation: Uses only numpy for matrix operations and Python for logic
ğŸ“š Training: Utilizes mini-batch gradient descent and backpropagation
ğŸ” Forward pass: Matrix multiplications and activation functions applied sequentially
ğŸ”„ Backward pass: Gradients computed using chain rule, weights updated accordingly
ğŸ¯ Loss function: Cross-entropy loss for multi-class classification
ğŸ“ Evaluation: Accuracy on test set, typically lower than more complex models but educational
ğŸš€ Advantages: Simplicity, full control over implementation, deep understanding of neural network mechanics
ğŸ”§ Challenges: Requires careful initialization and learning rate tuning, may converge slowly
