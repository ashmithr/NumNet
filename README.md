# NumNet
ğŸ§  Model: Basic feedforward neural network implemented from scratch for MNIST digit classification.

ğŸ“Š Input layer: 784 neurons (28x28 pixel images flattened)

ğŸ”® Hidden layer: Single layer with customizable number of neurons (e.g., 128 or 256)

ğŸ”¢ Output layer: 10 neurons (one for each digit 0-9)

ğŸ”— Activation functions: ReLU for hidden layer, softmax for output layer

ğŸ§® Implementation: Uses only numpy for matrix operations and Python for logic

ğŸ“š Training: Utilizes mini-batch gradient descent and backpropagation

ğŸ” Forward pass: Matrix multiplications and activation functions applied sequentially

ğŸ”„ Backward pass: Gradients computed using chain rule, weights updated accordingly

ğŸ“ Evaluation: Accuracy on test set is 82.5%

ğŸš€ Advantages: Simplicity, full control over implementation, deep understanding of neural network mechanics

ğŸ”§ Challenges: Requires careful initialization and learning rate tuning, may converge slowly
