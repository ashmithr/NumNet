# NumNet
🧠 Model: Basic feedforward neural network implemented from scratch for MNIST digit classification.
📊 Input layer: 784 neurons (28x28 pixel images flattened)
🔮 Hidden layer: Single layer with customizable number of neurons (e.g., 128 or 256)
🔢 Output layer: 10 neurons (one for each digit 0-9)
🔗 Activation functions: ReLU for hidden layer, softmax for output layer
🧮 Implementation: Uses only numpy for matrix operations and Python for logic
📚 Training: Utilizes mini-batch gradient descent and backpropagation
🔍 Forward pass: Matrix multiplications and activation functions applied sequentially
🔄 Backward pass: Gradients computed using chain rule, weights updated accordingly
🎯 Loss function: Cross-entropy loss for multi-class classification
📏 Evaluation: Accuracy on test set, typically lower than more complex models but educational
🚀 Advantages: Simplicity, full control over implementation, deep understanding of neural network mechanics
🔧 Challenges: Requires careful initialization and learning rate tuning, may converge slowly
